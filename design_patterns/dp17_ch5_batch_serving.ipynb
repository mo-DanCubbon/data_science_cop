{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f238e8e8-e5e4-4bbb-b888-fc1864855125",
   "metadata": {},
   "source": [
    "# Design Pattern 17 - Batch Serving(Chapter 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b59ac7-4bf4-48b2-99ca-2fb081742946",
   "metadata": {},
   "source": [
    "## Introduction to Design Pattern\n",
    "\n",
    "The previous pattern (16 Stateless Function), looked at serving a model through a stateless function. The aim was to quickly (low-latency) serve realtime predictions from a machine learning model. Many applications required one prediction as quickly as possible, for example:\n",
    "* credit card fraud detection\n",
    "* medical diagnosis\n",
    "* facial recognition for authentication\n",
    "The stateless function can then seamlessly use web/cloud infrastrcuture to scale up to serving millions of customers simultaneously.\n",
    "\n",
    "There are many application where we don't want or need to serve one result at a time as quickly as possible. Instead we need to do thouands or millions of predictions in large batches, sometimes according to a schedule. Examples of this include\n",
    "* personalised playlist on music app - can be created for each user in a batch\n",
    "* product recommendation - can be updated once a day/week based on recent trasactions, and then cached.\n",
    "* weather forecasts?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f4329-6ca1-4958-b60a-5268002cb202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b565be9-a5e0-4b6d-8a99-33416c3b85a4",
   "metadata": {},
   "source": [
    "## Key Features of solution\n",
    "* Run queries in a distributed compute and data space. Examples could include:\n",
    "  * Google Big Query. -see notebook example below\n",
    "  * REDIS AI - \n",
    "  * dask-ml - \n",
    "  * ray batch -  https://docs.ray.io/en/latest/data/batch_inference.html#batch-inference-home "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e6a3eb-1787-4654-8e6f-30169febca5e",
   "metadata": {},
   "source": [
    "key features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35877623-32b7-471b-923f-a93969e202d6",
   "metadata": {},
   "source": [
    "challenges and when not to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7522b0a-58fc-41b4-bc04-77a39a4178cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"final_wine_model.h5\")\n",
    "\n",
    "# Perform inference on the dataset\n",
    "predictions = loaded_model.predict(X_scaled)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Compare predicted classes with ground truth labels\n",
    "correct_predictions = np.sum(predicted_classes == y)\n",
    "accuracy = correct_predictions / len(y) * 100\n",
    "\n",
    "print(f\"Accuracy on the Wine dataset: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f07e1-7cb6-4958-bcd5-631d904e9620",
   "metadata": {},
   "source": [
    "## Example python implementation\n",
    "\n",
    "Example Google Cloud notebook: https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/05_resilience/batch_serving.ipynb\n",
    "\n",
    "The main problem is that the example is very Google Cloud centric, which not particularly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dbcc94-700d-4798-a317-0bb1ddbe771b",
   "metadata": {},
   "source": [
    "## Create sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53fee536-1385-40c5-9fc1-bc190984356c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 13:05:33.152665: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-14 13:05:33.963445: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 1.3230 - accuracy: 0.2920 - val_loss: 1.1560 - val_accuracy: 0.3793\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.1499 - accuracy: 0.3009 - val_loss: 1.0247 - val_accuracy: 0.4483\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.0087 - accuracy: 0.3717 - val_loss: 0.9081 - val_accuracy: 0.5172\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8930 - accuracy: 0.5575 - val_loss: 0.8047 - val_accuracy: 0.7241\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7899 - accuracy: 0.7257 - val_loss: 0.7129 - val_accuracy: 0.8276\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7019 - accuracy: 0.8496 - val_loss: 0.6331 - val_accuracy: 0.8966\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6213 - accuracy: 0.9027 - val_loss: 0.5619 - val_accuracy: 0.8966\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5518 - accuracy: 0.9381 - val_loss: 0.4954 - val_accuracy: 0.8966\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4885 - accuracy: 0.9469 - val_loss: 0.4361 - val_accuracy: 0.9655\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4319 - accuracy: 0.9646 - val_loss: 0.3816 - val_accuracy: 0.9655\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3792 - accuracy: 0.9735 - val_loss: 0.3321 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3328 - accuracy: 0.9735 - val_loss: 0.2889 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2906 - accuracy: 0.9735 - val_loss: 0.2495 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2527 - accuracy: 0.9735 - val_loss: 0.2151 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2204 - accuracy: 0.9735 - val_loss: 0.1859 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1921 - accuracy: 0.9823 - val_loss: 0.1620 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1676 - accuracy: 0.9823 - val_loss: 0.1409 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1481 - accuracy: 0.9823 - val_loss: 0.1243 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1313 - accuracy: 0.9823 - val_loss: 0.1105 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1175 - accuracy: 0.9823 - val_loss: 0.0991 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1058 - accuracy: 0.9823 - val_loss: 0.0896 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0959 - accuracy: 0.9912 - val_loss: 0.0816 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0877 - accuracy: 0.9912 - val_loss: 0.0753 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0807 - accuracy: 0.9912 - val_loss: 0.0697 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0743 - accuracy: 0.9912 - val_loss: 0.0648 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0685 - accuracy: 0.9912 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0639 - accuracy: 0.9912 - val_loss: 0.0573 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0598 - accuracy: 0.9912 - val_loss: 0.0539 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0558 - accuracy: 0.9912 - val_loss: 0.0514 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0525 - accuracy: 0.9912 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0492 - accuracy: 0.9912 - val_loss: 0.0469 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0464 - accuracy: 0.9912 - val_loss: 0.0451 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0437 - accuracy: 0.9912 - val_loss: 0.0436 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0415 - accuracy: 0.9912 - val_loss: 0.0419 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0391 - accuracy: 0.9912 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0370 - accuracy: 0.9912 - val_loss: 0.0387 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0350 - accuracy: 0.9912 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0332 - accuracy: 0.9912 - val_loss: 0.0359 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0298 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.0279 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Test loss: 0.017857268452644348\n",
      "Test accuracy: 1.0\n",
      "Model saved to disk.\n"
     ]
    }
   ],
   "source": [
    "## Create sample model\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define a ModelCheckpoint callback to save the best model during training\n",
    "checkpoint_callback = ModelCheckpoint(\"wine_model.h5\", save_best_only=True, save_weights_only=False)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[checkpoint_callback])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "# Save the trained model to disk\n",
    "model.save(\"final_wine_model.h5\")\n",
    "\n",
    "print(\"Model saved to disk.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955f2c4-f1f8-49fd-b43c-e5cf2c6a5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"final_wine_model.h5\")\n",
    "\n",
    "# Perform inference on the dataset\n",
    "predictions = loaded_model.predict(X_scaled)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Compare predicted classes with ground truth labels\n",
    "correct_predictions = np.sum(predicted_classes == y)\n",
    "accuracy = correct_predictions / len(y) * 100\n",
    "\n",
    "print(f\"Accuracy on the Wine dataset: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f315e-3758-4c89-a866-17c314760202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "\n",
    "# Step 1: Create a Ray Dataset from in-memory Numpy arrays.\n",
    "# You can also create a Ray Dataset from many other sources and file\n",
    "# formats.\n",
    "ds = ray.data.read_csv('wine_quality.csv')\n",
    "\n",
    "# Step 2: Define a Predictor class for inference.\n",
    "# Use a class to initialize the model just once in `__init__`\n",
    "# and re-use it for inference across multiple batches.\n",
    "class TFPredictor:\n",
    "    def __init__(self):\n",
    "        from tensorflow import keras\n",
    "        from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "        # Load a dummy neural network.\n",
    "        # Set `self.model` to your pre-trained Keras model.\n",
    "        input_layer = keras.Input(shape=(100,))\n",
    "        output_layer = keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "        self.model = keras.Sequential([input_layer, output_layer])\n",
    "\n",
    "    # Logic for inference on 1 batch of data.\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        # Get the predictions from the input batch.\n",
    "        return {\"output\": self.model(batch[\"data\"]).numpy()}\n",
    "\n",
    "# Use 2 parallel actors for inference. Each actor predicts on a\n",
    "# different partition of data.\n",
    "scale = ray.data.ActorPoolStrategy(size=2)\n",
    "# Step 3: Map the Predictor over the Dataset to get predictions.\n",
    "predictions = ds.map_batches(TFPredictor, compute=scale)\n",
    " # Step 4: Show one prediction output.\n",
    "predictions.show(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68703c-8ea3-4399-a1b1-e2725791900b",
   "metadata": {},
   "source": [
    "## Real world examples\n",
    "\n",
    "\n",
    "Try to include some actual/possible examples of where this DP could be used in a weather and climate context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01239aff-9f4e-43c4-b603-2da55882a8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf247c-46d8-4aa1-b8c5-3776596232e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
